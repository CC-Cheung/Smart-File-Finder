{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1552214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "FILE_PATH=os.getcwd()\n",
    "CODE_PATH= os.path.join(FILE_PATH, '..')\n",
    "DATA_PATH=os.path.join(CODE_PATH, 'data')\n",
    "MODELS_PATH=os.path.join(CODE_PATH, 'models')\n",
    "FINETUNED_PATH=os.path.join(MODELS_PATH, 'finetuned')\n",
    "\n",
    "PROCESSED_DATA_PATH=os.path.join(DATA_PATH, 'processed')\n",
    "RAW_DATA_PATH=os.path.join(DATA_PATH, 'raw')\n",
    "USED_DATA_PATH=os.path.join(DATA_PATH, 'used')\n",
    "\n",
    "tokenizer=None\n",
    "def pre_apply_chat_template(example):  \n",
    "    conversations = example[\"text\"]  \n",
    "    text = tokenizer.apply_chat_template(conversations, tokenize=False, add_generation_prompt=False)  \n",
    "    return {\"text\": text}  \n",
    "def pre_apply_chat_template_gen(example):  \n",
    "    conversations = example[\"text\"][:-1]  \n",
    "    text = tokenizer.apply_chat_template(conversations, tokenize=False, add_generation_prompt=True)  \n",
    "    return {\"text\": text}  \n",
    "def pre_apply_chat_template_gen_tokenize(example):  \n",
    "    conversations = example[\"text\"][:-1]  \n",
    "    text = tokenizer.apply_chat_template(conversations, tokenize=True, add_generation_prompt=True)  \n",
    "    return {\"text\": text}    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed530eed",
   "metadata": {},
   "source": [
    "### Out of Memory if GPU, Internal error if CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b15f2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.6: Fast Mistral patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Laptop GPU. Num GPUs = 1. Max memory: 7.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "!handles_.at(i) INTERNAL ASSERT FAILED at \"/pytorch/c10/cuda/CUDACachingAllocator.cpp\":396, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m     used_dataset = json.load(f)\n\u001b[32m      4\u001b[39m dataset = Dataset.from_list(used_dataset)  \n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[32m      7\u001b[39m     model_name=\u001b[33m\"\u001b[39m\u001b[33m/home/kids/Linux_Coding/Smart-File-Finder/models/finetuned/Mistral_d66ea65/Mistral_d66ea65_adapters\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     max_seq_length=\u001b[32m2048\u001b[39m,\n\u001b[32m      9\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# llm_int8_enable_fp32_cpu_offload = True,\u001b[39;00m\n\u001b[32m     11\u001b[39m     device_map=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Force CPU usage\u001b[39;00m\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env_jupyter/lib/python3.11/site-packages/unsloth/models/loader.py:376\u001b[39m, in \u001b[36mFastLanguageModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[39m\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m model, tokenizer = dispatch_model.from_pretrained(\n\u001b[32m    377\u001b[39m     model_name        = model_name,\n\u001b[32m    378\u001b[39m     max_seq_length    = max_seq_length,\n\u001b[32m    379\u001b[39m     dtype             = _get_dtype(dtype),\n\u001b[32m    380\u001b[39m     load_in_4bit      = load_in_4bit,\n\u001b[32m    381\u001b[39m     token             = token,\n\u001b[32m    382\u001b[39m     device_map        = device_map,\n\u001b[32m    383\u001b[39m     rope_scaling      = rope_scaling,\n\u001b[32m    384\u001b[39m     fix_tokenizer     = fix_tokenizer,\n\u001b[32m    385\u001b[39m     model_patcher     = dispatch_model,\n\u001b[32m    386\u001b[39m     tokenizer_name    = tokenizer_name,\n\u001b[32m    387\u001b[39m     trust_remote_code = trust_remote_code,\n\u001b[32m    388\u001b[39m     revision          = revision \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_peft \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    389\u001b[39m \n\u001b[32m    390\u001b[39m     fast_inference    = fast_inference,\n\u001b[32m    391\u001b[39m     gpu_memory_utilization = gpu_memory_utilization,\n\u001b[32m    392\u001b[39m     float8_kv_cache   = float8_kv_cache,\n\u001b[32m    393\u001b[39m     random_state      = random_state,\n\u001b[32m    394\u001b[39m     max_lora_rank     = max_lora_rank,\n\u001b[32m    395\u001b[39m     disable_log_stats = disable_log_stats,\n\u001b[32m    396\u001b[39m     *args, **kwargs,\n\u001b[32m    397\u001b[39m )\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     model.resize_token_embeddings(resize_model_vocab)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env_jupyter/lib/python3.11/site-packages/unsloth/models/mistral.py:400\u001b[39m, in \u001b[36mFastMistralModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_pretrained\u001b[39m(\n\u001b[32m    387\u001b[39m     model_name        = \u001b[33m\"\u001b[39m\u001b[33munsloth/mistral-7b-bnb-4bit\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    398\u001b[39m     **kwargs,\n\u001b[32m    399\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m FastLlamaModel.from_pretrained(\n\u001b[32m    401\u001b[39m         model_name        = model_name,\n\u001b[32m    402\u001b[39m         max_seq_length    = max_seq_length,\n\u001b[32m    403\u001b[39m         dtype             = dtype,\n\u001b[32m    404\u001b[39m         load_in_4bit      = load_in_4bit,\n\u001b[32m    405\u001b[39m         token             = token,\n\u001b[32m    406\u001b[39m         device_map        = device_map,\n\u001b[32m    407\u001b[39m         rope_scaling      = rope_scaling,\n\u001b[32m    408\u001b[39m         fix_tokenizer     = fix_tokenizer,\n\u001b[32m    409\u001b[39m         model_patcher     = FastMistralModel,\n\u001b[32m    410\u001b[39m         tokenizer_name    = tokenizer_name,\n\u001b[32m    411\u001b[39m         trust_remote_code = trust_remote_code,\n\u001b[32m    412\u001b[39m         **kwargs,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env_jupyter/lib/python3.11/site-packages/unsloth/models/llama.py:1787\u001b[39m, in \u001b[36mFastLlamaModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[39m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit: kwargs[\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m] = bnb_config\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fast_inference:\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m   1788\u001b[39m         model_name,\n\u001b[32m   1789\u001b[39m         device_map              = device_map,\n\u001b[32m   1790\u001b[39m         torch_dtype             = dtype,\n\u001b[32m   1791\u001b[39m         \u001b[38;5;66;03m# quantization_config     = bnb_config,\u001b[39;00m\n\u001b[32m   1792\u001b[39m         token                   = token,\n\u001b[32m   1793\u001b[39m         max_position_embeddings = max_position_embeddings,\n\u001b[32m   1794\u001b[39m         trust_remote_code       = trust_remote_code,\n\u001b[32m   1795\u001b[39m         attn_implementation     = \u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1796\u001b[39m         **kwargs,\n\u001b[32m   1797\u001b[39m     )\n\u001b[32m   1798\u001b[39m     model.fast_generate = model.generate\n\u001b[32m   1799\u001b[39m     model.fast_generate_batches = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env_jupyter/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    572\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    573\u001b[39m     )\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env_jupyter/lib/python3.11/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env_jupyter/lib/python3.11/site-packages/transformers/modeling_utils.py:4342\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4336\u001b[39m     config = \u001b[38;5;28mcls\u001b[39m._autoset_attn_implementation(\n\u001b[32m   4337\u001b[39m         config, use_flash_attention_2=use_flash_attention_2, torch_dtype=torch_dtype, device_map=device_map\n\u001b[32m   4338\u001b[39m     )\n\u001b[32m   4340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[32m   4341\u001b[39m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4342\u001b[39m     model = \u001b[38;5;28mcls\u001b[39m(config, *model_args, **model_kwargs)\n\u001b[32m   4344\u001b[39m \u001b[38;5;66;03m# Make sure to tie the weights correctly\u001b[39;00m\n\u001b[32m   4345\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env_jupyter/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:729\u001b[39m, in \u001b[36mMistralForCausalLM.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[32m    728\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config)\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = MistralModel(config)\n\u001b[32m    730\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_size = config.vocab_size\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env_jupyter/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:445\u001b[39m, in \u001b[36mMistralModel.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28mself\u001b[39m.layers = nn.ModuleList(\n\u001b[32m    442\u001b[39m     [MistralDecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config.num_hidden_layers)]\n\u001b[32m    443\u001b[39m )\n\u001b[32m    444\u001b[39m \u001b[38;5;28mself\u001b[39m.norm = MistralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m \u001b[38;5;28mself\u001b[39m.rotary_emb = MistralRotaryEmbedding(config=config)\n\u001b[32m    446\u001b[39m \u001b[38;5;28mself\u001b[39m.gradient_checkpointing = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    448\u001b[39m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env_jupyter/lib/python3.11/site-packages/unsloth/models/llama.py:1253\u001b[39m, in \u001b[36mLlamaRotaryEmbedding.__init__\u001b[39m\u001b[34m(self, dim, max_position_embeddings, base, device, config)\u001b[39m\n\u001b[32m   1250\u001b[39m \u001b[38;5;28mself\u001b[39m.current_rope_size = \u001b[38;5;28mmin\u001b[39m(\u001b[32m4\u001b[39m * \u001b[32m8192\u001b[39m, \u001b[38;5;28mself\u001b[39m.max_position_embeddings)\n\u001b[32m   1252\u001b[39m \u001b[38;5;66;03m# Build here to make `torch.jit.trace` work.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1253\u001b[39m \u001b[38;5;28mself\u001b[39m._set_cos_sin_cache(seq_len=\u001b[38;5;28mself\u001b[39m.current_rope_size, device=device, dtype=torch.get_default_dtype())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env_jupyter/lib/python3.11/site-packages/unsloth/models/llama.py:1268\u001b[39m, in \u001b[36mLlamaRotaryEmbedding._set_cos_sin_cache\u001b[39m\u001b[34m(self, seq_len, device, dtype)\u001b[39m\n\u001b[32m   1266\u001b[39m \u001b[38;5;66;03m# Different from paper, but it uses a different permutation in order to obtain the same calculation\u001b[39;00m\n\u001b[32m   1267\u001b[39m emb = torch.cat((freqs, freqs), dim=-\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1268\u001b[39m \u001b[38;5;28mself\u001b[39m.register_buffer(\u001b[33m\"\u001b[39m\u001b[33mcos_cached\u001b[39m\u001b[33m\"\u001b[39m, emb.cos().to(dtype=dtype, device=device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m), persistent=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1269\u001b[39m \u001b[38;5;28mself\u001b[39m.register_buffer(\u001b[33m\"\u001b[39m\u001b[33msin_cached\u001b[39m\u001b[33m\"\u001b[39m, emb.sin().to(dtype=dtype, device=device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m), persistent=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: !handles_.at(i) INTERNAL ASSERT FAILED at \"/pytorch/c10/cuda/CUDACachingAllocator.cpp\":396, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "with open(os.path.join(USED_DATA_PATH, 'used_dataset_sys_use_ass.json'), 'r') as f:\n",
    "    used_dataset = json.load(f)\n",
    "    \n",
    "dataset = Dataset.from_list(used_dataset)  \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"/home/kids/Linux_Coding/Smart-File-Finder/models/finetuned/Mistral_d66ea65/Mistral_d66ea65_adapters\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    "    # llm_int8_enable_fp32_cpu_offload = True,\n",
    "    device_map=\"cpu\",  # Force CPU usage\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "303e6387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f691b3b4ff4464cbb6b520e3bcf697a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'apply_chat_template'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m FastLanguageModel.for_inference(model) \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Apply the chat template to the dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m dataset_gen = dataset.map(pre_apply_chat_template_gen)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env_jupyter/lib/python3.11/site-packages/datasets/arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env_jupyter/lib/python3.11/site-packages/datasets/arrow_dataset.py:3079\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3074\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3075\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3076\u001b[39m         total=pbar_total,\n\u001b[32m   3077\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3078\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3079\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset._map_single(**dataset_kwargs):\n\u001b[32m   3080\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[32m   3081\u001b[39m                 shards_done += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env_jupyter/lib/python3.11/site-packages/datasets/arrow_dataset.py:3501\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[32m   3500\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3501\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m iter_outputs(shard_iterable):\n\u001b[32m   3502\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[32m   3503\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env_jupyter/lib/python3.11/site-packages/datasets/arrow_dataset.py:3475\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3473\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3474\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3475\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, apply_function(example, i, offset=offset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env_jupyter/lib/python3.11/site-packages/datasets/arrow_dataset.py:3398\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3396\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3397\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3398\u001b[39m processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n\u001b[32m   3399\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mpre_apply_chat_template_gen\u001b[39m\u001b[34m(example)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpre_apply_chat_template_gen\u001b[39m(example):  \n\u001b[32m     23\u001b[39m     conversations = example[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m][:-\u001b[32m1\u001b[39m]  \n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     text = tokenizer.apply_chat_template(conversations, tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m)  \n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text}\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'apply_chat_template'"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) \n",
    "\n",
    "# Apply the chat template to the dataset\n",
    "dataset_gen = dataset.map(pre_apply_chat_template_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd47f26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b8957b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m inputs = tokenizer(\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     pre_apply_chat_template_gen(dataset_gen[\u001b[32m0\u001b[39m],tokenizer)[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      3\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# add_special_tokens=False, \u001b[39;00m\n\u001b[32m      5\u001b[39m     padding=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      6\u001b[39m ).to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m text_streamer = TextStreamer(tokenizer, skip_prompt = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      8\u001b[39m _ = model.generate(inputs[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m], attention_mask = inputs[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m], streamer=text_streamer, max_new_tokens = \u001b[32m128\u001b[39m, pad_token_id = tokenizer.eos_token_id)\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset_gen' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    pre_apply_chat_template_gen(dataset_gen[0],tokenizer)['text'],\n",
    "    return_tensors=\"pt\",\n",
    "    # add_special_tokens=False, \n",
    "    padding=True,\n",
    ").to(\"cuda\")\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(inputs['input_ids'], attention_mask = inputs['attention_mask'], streamer=text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)\n",
    "_ = model.generate(torch.tensor([pre_apply_chat_template_gen_tokenize(dataset_gen[0],tokenizer)['text']]).to(\"cuda\"), \n",
    "                    streamer=text_streamer,\n",
    "                    max_new_tokens = 128, \n",
    "                    pad_token_id = tokenizer.eos_token_id)\n",
    "\n",
    "print(_)\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00aa833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env_jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
